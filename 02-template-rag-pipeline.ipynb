{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy2f3Wn8rnRqw0iKhBxPba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/poschat-dssg/blob/main/02_template_rag-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using RAG with OpenAI Models\n",
        "\n",
        "This notebook outlines the basic setup for using RAG (**R**etrieval **A**ugmented **G**eneration) on a PDF document when making API calls to OpenAI's models from within a jupyter notebook.\n",
        "\n",
        "It is meant as a \"template\" notebook - it has simple functionalities laid out that can be built upon later to make more complex systems."
      ],
      "metadata": {
        "id": "fWzx5aJIy6oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0. Some Background\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a method that enhances text generation models by retrieving relevant information from a large dataset or knowledge base before generating a response. It looks for what parts of a knowledge set contain relevant information to the conservation, and adds those relevant bits of information into the context when calling a model.\n",
        "\n",
        "The RAG process involves:\n",
        "1. **Retrieval**: Querying a (usually) large dataset - too large to fit entirely into context - to find relevant documents or snippets.\n",
        "2. **Augmentation**: The retrieved information is combined with the query to provide additional context.\n",
        "3. **Generation**: A text generation model uses the augmented context to generate a more informed and accurate response.\n",
        "\n",
        "It's advantageous because it allows for a much larger knowledge base to be accessible to the model. If only relevant parts of the knowledge are retrieved and added to the context, you avoid context limits, but ensure that the model has access to information that it needs.\n",
        "\n",
        "For a more complete introductory guide to RAG, I highly recommend these resources:\n",
        "1. https://medium.com/@amodwrites/understanding-retrieval-augmented-generation-a-simple-guide-d638ac92c123\n",
        "2. https://blog.gopenai.com/introduction-to-retrieval-augmented-generation-rag-a-beginners-guide-35db961402ca (a bit more in-depth)"
      ],
      "metadata": {
        "id": "XuX6uDGc-kkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Setup\n",
        "\n",
        "Before we can start writing code to make the calls, we need to do some setup. This will include installing packages, importing packages, and giving ourself model access with our API key.\n",
        "\n",
        "This code will also make use of a \"sample\" PDF. The sample PDF used in this code is from here: https://unec.edu.az/application/uploads/2014/12/pdf-sample.pdf. It can be downloaded and uploaded into this session for use in this code."
      ],
      "metadata": {
        "id": "tC2JVFal06No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary packages\n",
        "Some packages need to be installed into our current environment before we're able to use them.\n",
        "This cell will print out some short messages as it works to collect those packages for us."
      ],
      "metadata": {
        "id": "BttYl2T6zbBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q openai\n",
        "!pip install -q pypdf\n",
        "!pip install -q langchain\n",
        "!pip install -q langchain-openai\n",
        "!pip install -q langchain-community\n",
        "!pip install -q chromadb\n",
        "!pip install -q langchain-chroma"
      ],
      "metadata": {
        "id": "hp5XzOIjzkSV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing those packages\n",
        "Next, we'll import the packages that we need to use in this code.\n",
        "\n",
        "Langchain is an extremely popular python package for building RAG systems. We'll need to import a lot from langchain, which we'll explain in more depth later."
      ],
      "metadata": {
        "id": "imVK3MqnzvLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "blLI-Zkrznw-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding in our API key\n",
        "\n",
        "The final setup step we need is to add our API key, so that OpenAI will give us access to their models.\n",
        "When running the cell below, a text box should open where you can paste your API key in and hit \"enter\", granting access."
      ],
      "metadata": {
        "id": "WQAsfLXO0G1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXyWrddj0n7u",
        "outputId": "c8da9b5f-6af6-423d-f4db-00ec3a056ac8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Set up our PDF as a vectorstore\n",
        "\n",
        "Instead of directly reading in all text from the PDF, we're going to turn it into a vector store with langchain that we can then retrieve information from.\n",
        "\n",
        "This has multiple steps."
      ],
      "metadata": {
        "id": "Paj0_92i1aZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll load in the PDF directly with langchain's PyPDFLoader.\n",
        "\n",
        "* `PyPDFLoader`: This class is part of LangChain and is designed to handle PDF documents. It abstracts away the complexities of reading and processing PDFs.\n",
        "loader = PyPDFLoader('pdf-sample.pdf'): Creates an instance of the PyPDFLoader class, initializing it with the path to the PDF file (pdf-sample.pdf).\n",
        "* `documents = loader.load()`: Loads the content of the PDF file. The load() method reads the PDF and converts it into a format that LangChain can work with, typically a list of Document objects where each Document represents a chunk of text from the PDF."
      ],
      "metadata": {
        "id": "841PqIO7PoNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bVjkYmgaywje"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader('pdf-sample.pdf')\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we split the text into chunks.\n",
        "\n",
        "* `RecursiveCharacterTextSplitter`: This is a utility from LangChain used to split large documents into smaller chunks. This is necessary because large documents can be too big to process in a single call to a language model.\n",
        "* `chunk_size=200`: Specifies the maximum number of characters each chunk should contain.\n",
        "* `chunk_overlap=50`: Specifies the number of characters that should overlap between consecutive chunks. This overlap ensures that important information that might be split between chunks is preserved.\n",
        "* `text_splitter.split_documents(documents)`: Splits the loaded documents into smaller chunks based on the specified chunk_size and chunk_overlap. The result, docs, is a list of smaller document chunks.\n",
        "\n",
        "Usually, a larger chunk size is chosen. We're making our chunk size and chunk overlap small, because our PDF document was short."
      ],
      "metadata": {
        "id": "V8prFNVNP_5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "-2gfIG55P2sb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We turn these documents into a vector store with Chroma.\n",
        "\n",
        "* **Chroma** is a vector store implementation. A vector store is used to store and index vector embeddings of text documents, enabling efficient similarity searches.\n",
        "* `OpenAIEmbeddings`: This class uses OpenAI's embeddings API to convert text into numerical vector representations (embeddings).\n",
        "* `from_documents`: A method that creates a Chroma vector store from a list of documents.\n",
        "* `docs`: The list of document chunks created in the previous step."
      ],
      "metadata": {
        "id": "lcNvtwYzQhZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "OENO4xJVP_UG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we set up a retriever so we can search the vector store.\n",
        "\n",
        "* `as_retriever()`: This method converts the vector store into a retriever. A retriever is a component that takes a query, converts it into an embedding, and then finds and returns the most similar document embeddings from the vector store.\n",
        "* `retriever`: The resulting retriever can now be used to find and retrieve relevant document chunks based on a query."
      ],
      "metadata": {
        "id": "p4N_O_ArQ8h0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "BWIVBe0cROgm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Set up RAG system\n",
        "\n",
        "Finally, we'll set up a chain that makes use of this retriever when we ask a question to the model.\n",
        "\n",
        "We start by specifying the OpenAI model that we want to use, gpt-3.5-turbo here."
      ],
      "metadata": {
        "id": "FfGgURBi2I7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "zRTIhk0DIqEG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a system prompt. This is an example system prompt provided by langchain, which guides how the model should respond, and provides a place to input context extracted from the documents."
      ],
      "metadata": {
        "id": "7G1paMyLRfnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Incorporate the retriever into a question-answering chain.\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ],
      "metadata": {
        "id": "5y1KOnEbLOny"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a prompt template for our user. This will include both the system prompt, and a flexible spot for us to ask whatever questions we have."
      ],
      "metadata": {
        "id": "ckoK3uBTRw2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "kOfEZuzjRz8-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we set up chains to make use of our llm, prompt, and retriever.\n",
        "\n",
        "1. We first set up a question/answer chain. This chain simply includes a prompt and an llm, and the pipeline here passes the prompt into the LLM. This chain doesn't yet call on our documents.\n",
        "\n",
        "2. The rag_chain combines the question/answer chain with our retriever to complete the pipeline, by passing in the retrieved information into the question/answer chain."
      ],
      "metadata": {
        "id": "nJTt9SHcSDul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "r7RjI8z4Ru_-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `invoke` to get an answer from our model."
      ],
      "metadata": {
        "id": "GnWJcus_SlFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"What are the advantages of PDF's?\"})\n",
        "response[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ENY4QUvPF7_d",
        "outputId": "ff2df9cc-3613-4326-82c7-40cada8304e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The advantages of PDF files include the ability to display documents exactly as created regardless of fonts, software, and operating systems, as well as the preservation of fonts, graphics, and formatting when sharing files. Additionally, PDF files always print correctly on any printing device.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see that the answer it gives was taken directly from the information in the PDF!"
      ],
      "metadata": {
        "id": "a8II9DuZSpCC"
      }
    }
  ]
}
